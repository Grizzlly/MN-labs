\documentclass{exam}

\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{color}
\usepackage{fancyvrb}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ %
	backgroundcolor=\color{white},		% choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
	basicstyle=\small\ttfamily,		% the size of the fonts that are used for the code
	breakatwhitespace=false,			% sets if automatic breaks should only happen at whitespace
	breaklines=true,					% sets automatic line breaking
	captionpos=b,						% sets the caption-position to bottom
	columns=fullflexible,
	commentstyle=\color{mygreen},		% comment style
	deletekeywords={...},				% if you want to delete keywords from the given language
	escapeinside={\%*}{*)},			% if you want to add LaTeX within your code
	extendedchars=true,				% lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
	frame=single,						% adds a frame around the code
	keepspaces=true,					% keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
	keywordstyle=\color{blue},			% keyword style
	language=Octave,					% the language of the code
	morekeywords={*,...},				% if you want to add more keywords to the set
	%   numbers=left,						% where to put the line-numbers; possible values are (none, left, right)
	%   numbersep=6pt,						% how far the line-numbers are from the code
	%   numberstyle=\tiny\color{mygray},	% the style that is used for the line-numbers
	rulecolor=\color{black},			% if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
	showspaces=false,					% show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
	showstringspaces=false,			% underline spaces within strings only
	showtabs=false,					% show tabs within strings adding particular underscores
	stepnumber=1,						% the step between two line-numbers. If it's 1, each line will be numbered
	stringstyle=\color{mymauve},		% string literal style
	tabsize=2,							% sets default tabsize to 2 spaces
	title=\lstname						% show the filename of files included with \lstinputlisting; also try caption instead of title
}

% Creates a new command to include a perl script, the first parameter is the filename of the script (without .pl), the second parameter is the caption
\newcommand{\octavescript}[2]{
	\lstinputlisting[caption=#2]{#1}
}

\newcommand{\MNLab}{Laborator\ \#4}
\newcommand{\MNLabTitle}{Eliminare gaussiană cu pivotare totală și scalare. Algoritmul Thomas pentru rezolvarea sistemului 3-diagonal.}
\newcommand{\MNLabTitleHeader}{Eliminare gaussiană}
\newcommand{\MNAuthor}{Andrei STAN, Florin Pop, Mihaela Andreea-Vasile}

\renewcommand{\contentsname}{Cuprins}
\renewcommand{\figurename}{Figura}
\renewcommand{\refname}{Referințe}

\setlength{\parskip}{0.5\baselineskip}

\graphicspath{{./img/}}

\title{
	\textmd{\textbf{\MNLabTitle}}
	\author{}
	\date{}
}

\pagestyle{headandfoot}

\header{Metode Numerice}
{\MNLabTitleHeader, Pagina \thepage\ din \numpages}
{2025}
\footer{Facultatea de Automatică și Calculatoare}{}{Pagina \thepage\ din \numpages}

\begin{document}

\begin{coverpages}
	\maketitle
	\thispagestyle{empty}
	\tableofcontents
\end{coverpages}

\section{Obiective laborator}

În urma parcurgerii acestui laborator, studentul va fi capabil să:
\begin{itemize}
	\item Transforme o matrice nesingulară într-o matrice superior sau inferior triunghiulară folosind una din metodele prezentate de eliminare gaussiană;
	\item Implementeze algoritmii de eliminare gaussiană prezentați;
	\item Implementeze algoritmul Thomas.
\end{itemize}

\section{Noțiuni teoretice}

Eliminarea Gaussiană sau \textit{reducerea liniilor} (engl. \textit{row-reduction})
este un algoritm fundamental în algebra liniară. Acesta poate fi folosit pentru
rezolvarea sistemelor liniare, găsirea rang-ului unei matrice, inversarea unei
matrice, sau calcularea determinantului.

Algoritmul se bazează pe efectuarea unor operații elementarea pentru a
permuta, scala și combina liniile unei matrici. Gândindu-ne la un sistem de
ecuații liniare, acesta nu se schimbă dacă facem aceste operații. Dacă mai
adăugăm și permutarea coloanelor, atunci soluția sistemului se schimbă doar prin
aceste permutări, ușor de inversat. Obiecivul este să aducem matricea la
\textit{forma eșalon} sau \textit{forma eșalon redusă}.

Așadar, operațiile elementare cu liniile sunt:

\begin{enumerate}
	\item permutarea liniilor;
	\item înmulțirea unei linii cu un scalar nenul;
	\item adunarea unei linii scalate la altă linie.
\end{enumerate}

\subsection{Forma eșalon a unei matrice}

Pentru fiecare linie dintr-o matrice, numim prima valoare diferită de zero
\textit{coeficient principal} sau \textit{pivot}. Dacă doi pivoți se găsesc pe
aceeași coloană, putem mereu să facem pe unul dintre ei 0. Ne folosim apoi de
permutari, pentru a aranja liniile în așa fel încât pivotul unei linii să fie
la dreapta pivotului liniei anterioare.

De exemplu, matricea de mai jos este în forma eșalon. Observați cum toate
liniile pline de 0 sunt jos.

\begin{equation*}
	\begin{bmatrix}
		0 & 1 & 2 & 3 \\
		0 & 0 & 4 & 5 \\
		0 & 0 & 0 & 0 \\
		0 & 0 & 0 & 0
	\end{bmatrix}
\end{equation*}

În plus, putem spune despre o matrice că este în
\textit{forma eșalon redusă} dacă toți pivoții sunt 1 (lucru care se poate
obține prin scalare) și toate celelalte elemente de pe coloana unui pivot sunt 0
(lucru care se poate obține prin adunarea unei linii scalate la altă linie).

\subsection{Matrici elementare}

O matrice elementară este o matrice care se obține din matricea identitate
prin efectuarea unei operații elementare cu liniile.

\textbf{Matricea de permutare.} Interschimbă liniile $i$ și $j$.

\begin{equation*}
	\begin{split}
		P_{ij} = \begin{bmatrix}
			         1 &        &   &        &   &        &   \\
			           & \ddots &   &        &   &        &   \\
			           &        & 0 &        & 1 &        &   \\
			           &        &   & \ddots &   &        &   \\
			           &        & 1 &        & 0 &        &   \\
			           &        &   &        &   & \ddots &   \\
			           &        &   &        &   &        & 1
		         \end{bmatrix}
	\end{split}
	\quad
	\begin{split}
		 & \text{Construire:}      \\
		 & P = eye(n)              \\
		 & P (i, i) = P (j, j) = 0 \\
		 & P (i, j) = P (j, i) = 1
	\end{split}
\end{equation*}

Proprietăți:
\begin{itemize}
	\item $P = P^{-1} = P^T \implies P$ ortogonală
	\item $\det P = -1 \implies \det (PA) = -\det A$
\end{itemize}

\textbf{Matricea de scalare.} Înmulțește linia $i$ cu un scalar $\alpha$.

\begin{equation*}
	\begin{split}
		S_{i}(\alpha) = \begin{bmatrix}
			                1 &        &        &        &   \\
			                  & \ddots &        &        &   \\
			                  &        & \alpha &        &   \\
			                  &        &        & \ddots &   \\
			                  &        &        &        & 1
		                \end{bmatrix}
	\end{split}
	\quad
	\begin{split}
		 & \text{Construire:} \\
		 & S = eye(n)         \\
		 & S (i, i) = \alpha
	\end{split}
\end{equation*}

Proprietăți:
\begin{itemize}
	\item $D_i(m)^{-1} = D_i (\frac{1}{m})$
	\item $\det S = \alpha \implies \det (SA) = \alpha \det A$
\end{itemize}

\textbf{Matricea de adunare.} Adună linia $i$ la linia $j$ înmulțită cu un
scalar $\alpha$.

\begin{equation*}
	\begin{split}
		E_{ij}(\alpha) = \begin{bmatrix}
			                 1 &        &        &        &   &        &   \\
			                   & \ddots &        &        &   &        &   \\
			                   &        & 1      &        &   &        &   \\
			                   &        &        & \ddots &   &        &   \\
			                   &        & \alpha &        & 1 &        &   \\
			                   &        &        &        &   & \ddots &   \\
			                   &        &        &        &   &        & 1
		                 \end{bmatrix}
	\end{split}
	\quad
	\begin{split}
		 & \text{Construire:} \\
		 & E = eye(n)         \\
		 & E (i, j) = \alpha
	\end{split}
\end{equation*}

Proprietăți:

\begin{itemize}
	\item $E_{ij}(\alpha)^{-1} = E_{ij}(-\alpha)$
	\item $\det E = 1 \implies \det (EA) = \det A$
\end{itemize}

\subsubsection{Descompunerea LU}

Prin aplicare unor astfel de matrice și ajungând la forma eșalon, am ajuns
la o matrice superior triunghiulară.
\begin{align*}
	T_nT_{n-1}\ldots T_1 A & = U                                             \\
	A                      & = T_1^{-1}T_2^{-1}\ldots T_n^{-1}U              \\
	A                      & = LU, \quad L = T_1^{-1}T_2^{-1}\ldots T_n^{-1}
\end{align*}

\subsubsection{Calcularea determinantului}

După cum am spus la primul laborator, determinantul se calculează în $O(n!)$
operații. Eliminarea Gaussiană are complexitate $O(n^3)$.

Pentru o matrice triunghiulară $U$ avem că $\det U = \prod_{i=1}^{n} u_{ii}$.
\begin{equation*}
	\det A = \frac{\det U}{\prod_{i = 1}^{n} \det T_i}
\end{equation*}

\subsection{Eliminarea gaussiană - G}

\textbf{Exemplu.}

\begin{equation*}
	\begin{bmatrix}
		1 & 3  & 1  & 9  \\
		1 & 1  & -1 & 1  \\
		3 & 11 & 8  & 35
	\end{bmatrix} \to
	\begin{bmatrix}
		1 & 3  & 1  & 9  \\
		0 & -2 & -2 & -8 \\
		0 & 2  & 5  & 8
	\end{bmatrix} \to
	\begin{bmatrix}
		1 & 3  & 1  & 9  \\
		0 & -2 & -2 & -8 \\
		0 & 0  & 3  & 0
	\end{bmatrix}
\end{equation*}

Algoritmul în MATLAB poate fi gândit astfel:

\begin{enumerate}
	\item Folosesc un $p$ ca să ma plimb pe coloane, până când $p$ este egal
	      cu numărul de linii sau de coloane.
	\item Pivotul meu va fi $a_{pp}$.
	\item Iau restul de elemente de pe coloană folosind un index $i$ și calculez
	      $\mu_{ip} = \frac{a_{ip}}{a_{pp}}$. Acești coeficienți îi pun în
	      matricea $T$, $T(i, p) = -\mu_{ip}$.
	\item Calculez produsul $TA$ și continui cu următoarea coloană.
\end{enumerate}

% \newpage
% \octavescript{./src/G.m}{}

\begin{algorithm}
	\caption{Eliminare gaussiană}
	\begin{algorithmic}[1]
		\State \( [m, n] \gets \text{dimensiunea matricei } A \)
		\State \( \text{maxP} \gets \min(m, n) \) \Comment{Determinăm numărul de operații de eliminare}

		\For{$p = 1$ to $\text{maxP}$}
		\State \( T \gets I_m \) \Comment{Inițializăm matricea de transformare}
		\State \( \mu \gets A(p+1:m, p) / A(p, p) \) \Comment{Calculăm coeficienții}
		\State \( T(p+1:m, p) \gets -\mu \) \Comment{Actualizăm transformarea}
		\State \( A \gets T \cdot A \) \Comment{Aplicăm transformarea}
		\EndFor
	\end{algorithmic}
\end{algorithm}

De notat că nu este nevoie să aducem matricea la forma eșalon redusă pentru
a rezolva un sistem, trebuie doar să fie triunghiulară. În codul de mai sus, $A$
este matricea extinsă a sistemului. Mai mult, am introduce erori de calcul în
plus din cauza efectuării mai multor operații.

Alt lucru de observat este că dacă pivotul selectat este 0, atunci
programul "crapă" (womp-womp). Pentru a rezolva acest lucru introducem
conceptul de \textit{pivotare}.

\subsection{Pivotare}

Pivotarea constă în folosirea permutărilor și a scalării pentru a alege
cel mai bun pivot. Este clar că eliminearea Gauss poate fi aplicată pe orice
matrice, deci în continuare discuția se va baza pe analiză numerică.

\subsubsection{De ce este pivotarea importantă?}

Fie un calculator care folosește aritmetică în virgulă mobilă, cu 3
zecimale, cu trunchiere. Fie următorul sistem:

\begin{equation*}
	\begin{bmatrix}
		0.913 & 0.659 \\
		0.780 & 0.563
	\end{bmatrix}
	\begin{bmatrix}
		x_1 \\
		x_2
	\end{bmatrix}
	=
	\begin{bmatrix}
		0.254 \\
		0.217
	\end{bmatrix}
\end{equation*}

\begin{equation*}
	\mu = \frac{0.780}{0.913} = 0.854 \quad \text{(trunchiere)}
\end{equation*}

Sistemul devine:

\begin{equation*}
	\begin{bmatrix}
		0.913 & 0.659 \\
		0     & 0.001
	\end{bmatrix}
	\begin{bmatrix}
		x_1 \\
		x_2
	\end{bmatrix}
	=
	\begin{bmatrix}
		0.254 \\
		0.001
	\end{bmatrix}
\end{equation*}

Soluțiile sunt $x_2 = 1$ și $x_1 = \frac{0.254 - 0.659}{0.913} = -0.417$.

Comparăm soluția găsită, $x_* = \begin{bmatrix} -0.417 \\ 1 \end{bmatrix}$,
cu soluția reală, $x = \begin{bmatrix} -1 \\ 1 \end{bmatrix}$:

\begin{align*}
	||x - x_*||               & = ||\begin{bmatrix} -0.557 \\ 0 \end{bmatrix}|| = 0.557 \\
	||x||                     & = ||\begin{bmatrix} -1 \\ 1 \end{bmatrix}|| = \sqrt{2}  \\
	\frac{||x - x_*||}{||x||} & = \frac{0.557}{\sqrt{2}} = 0.39
\end{align*}

Eroarea relativă este de aproape 40\%. Foarte mare! Dacă calculăm reziduul,
am vedea că el este totuși mic, dar pe noi ne interesează soluția. Prezentăm
următoarele strategii:

\begin{itemize}
	\item Pivotare parțială - GPP;
	\item Pivotare parțială cu pivot scalat - GPPS;
	\item Pivotare totală - GPT.
\end{itemize}

\subsubsection{Pivotare parţială - GPP}

Pivotarea parțială are loc numai prin permutarea liniilor. La pasul $p$ al
algoritmului se aduce în locul pivotului cel mai mare element din coloană de sub
acesta.

Algoritmul în MATLAB se gândește exact la fel, numai că înaintea calculării
coeficienților $\mu_{ip}$ se face permutarea liniilor.

\begin{algorithm}
	\caption{GPP}
	\begin{algorithmic}[1]
		\State \( [m, n] \gets \text{dimensiunea matricei } A \)
		\State \( \text{maxP} \gets \min(m, n) \) \Comment{Determinăm numărul de operații de eliminare}
        
		\For{$p = 1$ to $\text{maxP}$}
		\State \( \text{idx} \gets \max |A(p : m, p)| \) \Comment{Determinăm pivotul maxim}
		\State \( \text{idx} \gets \text{idx} + p - 1 \) \Comment{Ne trebuie index-ul din toată coloana, nu doar de la p : m}
        \\
		\State \( P \gets I_m \)
		\State \( P(p, p) \gets 0 \) 
            \State \( P(idx, idx) \gets 0 \)
            \State \( P(p, idx) \gets 1 \)
            \State \( P(idx, p) \gets 1 \)
        \\
            \State \( A \gets P \cdot A\)
        \\
            \State \(T \gets I_m\)
            \State \( \mu \gets A(p+1:m, p) / A(p, p) \)
		\State \( T(p+1:m, p) \gets -\mu \) 
		\State \( A \gets T \cdot A \)
		\EndFor
	\end{algorithmic}
\end{algorithm}

%\octavescript{./src/GPP.m}{}

Este de observat că această metodă de pivotare încă nu rezolvă eroarea din
sistemul de mai sus, nu se efectuează nicio permutare. Totuși, în general, este
suficientă pentru a face erorile rezonabile. Dacă pivotul ajunge să fie 0,
atunci matricea este singulară.

\subsection{Pivotare parţială cu pivot scalat - GPPS}

Această tehnică este similară cu precedenta, doar că definim la început un
factor de scalare pentru fiecare linie $i$, factor de forma:

\begin{equation*}
	s_i = \max_{j=p:n}{\{|a_{ij}|\}} \quad \text{sau} \quad s_i = \sum_{j=p}^{n}{|a_{ij}|}
\end{equation*}

Dacă $s_i = 0$ atunci matricea este singulară.

Prin această metodă interschimbăm liniile în funcție de elementul cel mai
mare relativ la celelelte elemente de pe linie. Practic, la fiecare pas $p$,
vom căuta întregul $i_p$ astfel încât:

\begin{equation*}
	\frac{|a_{i_p p}|}{s_{i_p}} = \max_{i=p:n}{\frac{|a_{i p}|}{s_i}}
\end{equation*}

De exemplu, considerând sistemul
$\begin{bmatrix}
		1 & 10000  \\
		1 & 0.0001
	\end{bmatrix}
	\begin{bmatrix}
		x \\
		y
	\end{bmatrix}
	=
	\begin{bmatrix}
		10000 \\
		1
	\end{bmatrix}
$, folosind GPP nu ar trebui sa interschimbăm liniile, dar avem exact aceeași
problemă cu stabilitatea numerică. În consecință, interschimbăm liniile pentru
că 1 este foarte mic relativ la 10000.

Algorimtul în MATLAB este similar cu cel de la GPP, dar introducem
calculul factorului de scalare.

\begin{algorithm}[H]
	\caption{GPPS}
	\begin{algorithmic}[1]
		\State \( [m, n] \gets \text{dimensiunea matricei } A \)
		\State \( \text{maxP} \gets \min(m, n) \) \Comment{Determinăm numărul de operații de eliminare}
        
		\For{$p = 1$ to $\text{maxP}$}
		\State \( s_{factors} \gets \max |A(p : m, p : n-1)| \) \Comment{Determinăm pivotul maxim pe linii}
		\State \( \text{fractions} \gets A(p : m, p) \text{./} s_{factors}\)
        \\
            \State \( \text{idx} \gets \max |\text{fractions}| \) 
		\State \( \text{idx} \gets \text{idx} + p - 1 \)
		\State \( P \gets I_m \)
		\State \( P(p, p) \gets 0 \) 
            \State \( P(idx, idx) \gets 0 \)
            \State \( P(p, idx) \gets 1 \)
            \State \( P(idx, p) \gets 1 \)
        \\
            \State \( A \gets P \cdot A\)
        \\
            \State \(T \gets I_m\)
            \State \( \mu \gets A(p+1:m, p) / A(p, p) \)
		\State \( T(p+1:m, p) \gets -\mu \) 
		\State \( A \gets T \cdot A \)
		\EndFor
	\end{algorithmic}
\end{algorithm}

%\octavescript{./src/GPPS.m}{}

\subsection{Pivotare totală - GPT}

Cea mai bună stabilitate numerică se obține atunci când pivotul este cel
mai mare element în modul, din toata submatricea rămasă. Această tehnică se
numește pivotare totală. Totuși, în practică nu se folosește deoarece trebuie
ca la fiecare iterație să parcurgi toată submatricea. Practic, costurile
sunt mai mari decât beneficiile.

Pentru că acum trebuie să permutăm și coloanele, trebuie să ținem minte
aceste permutări. Acest lucru se datorează faptului că se schimbă ordinea
necunoscutelor. Pentru aceste permutări, ne folosim tot de o matrice $P$ numai
că o vom aplica la dreapta.

Algoritmul în MATLAB este similar cu cel de la GPP, doar că modificăm
căutarea maximului.

\begin{algorithm}[h!]
	\caption{GPT}
	\begin{algorithmic}[1]
		\State \( [m, n] \gets \text{dimensiunea matricei } A \)
		\State \( \text{maxP} \gets \min(m, n) \) 
        \Comment{Determinăm numărul de operații de eliminare}
            \State \(\text{rightPerms} \gets I_n\)
            
		\For{$p = 1$ to $\text{maxP}$}
            \State \( \text{idx} \gets \max |A(p : m, p : n - 1)| \) \Comment{Determinăm pivotul maxim din întreaga matrice}
            \State \(\text{A'} \gets A(p : m, p : n - 1)\)
            \State \([row, col] \gets \text{dimensiunea matricei } A'\)
            \State \(\text{row} \gets \text{row} + p - 1\)
            \State \(\text{col} \gets \text{col} + p - 1\)
	\\
		\State \( P \gets I_m \)
		\State \( P(p, p) \gets 0 \) 
            \State \( P(idx, idx) \gets 0 \)
            \State \( P(p, idx) \gets 1 \)
            \State \( P(idx, p) \gets 1 \)
        \\
            \State \(\text{rightPerms} \gets \text{rightPerms} * P\)
            \State \( A \gets P \cdot A\)
        \\
            \State \( P \gets I_m \)
		\State \( P(p, p) \gets 0 \) 
            \State \( P(idx, idx) \gets 0 \)
            \State \( P(p, idx) \gets 1 \)
            \State \( P(idx, p) \gets 1 \)
        \\
            \State \( A \gets P \cdot A\)
        \\
            \State \(T \gets I_m\)
            \State \( \mu \gets A(p+1:m, p) / A(p, p) \)
		\State \( T(p+1:m, p) \gets -\mu \) 
		\State \( A \gets T \cdot A \)
		\EndFor
	\end{algorithmic}
\end{algorithm}
%\octavescript{./src/GPT.m}{}

\newpage

Pentru că $A$ este matricea extinsă trebuie să avem grijă cu indicii.
Ultima comandă de \verb|display| demonstrează restabilirea ordinii inițiale a
necunoscutelor.

\subsection{Algoritmul Thomas pentru rezolvarea sistemului 3-diagonal}

Un sistem tridiagonal are ecuații de forma:
\begin{equation*}
	a_i x_{i - 1} + b_i x_i + c_i x_{i + 1} = d_i, \quad a_0 = 0, \quad c_n = 0
\end{equation*}

Iar sub forma matriceală:
\begin{equation*}
	\begin{bmatrix}
		b_1 & c_1 &        &        & 0         \\
		a_2 & b_2 & c_2    &        &           \\
		    & a_3 & b_3    & \ddots &           \\
		    &     & \ddots & \ddots & c_{n - 1} \\
		0   &     &        & a_n    & b_n       \\
	\end{bmatrix}
	\begin{bmatrix}
		x_1    \\
		x_2    \\
		x_3    \\
		\vdots \\
		x_n    \\
	\end{bmatrix}
	=
	\begin{bmatrix}
		d_1    \\
		d_2    \\
		d_3    \\
		\vdots \\
		d_n    \\
	\end{bmatrix}
\end{equation*}

Un astfel de sistem este un caz special al eliminării Gaussiene și poate fi
rezolvat în doar $O(n)$ operații. Astfel de matrici apar în special la calculul
spline-urilor cubice.

Fie primele 2 ecuații ale unui sistem tridiagonal:
\begin{align*}
	 & b_1 x_1 + c_1 x_2           & = d_1 \\
	 & a_2 x_1 + b_2 x_2 + c_2 x_3 & = d_2
\end{align*}

Facem primul pas din eliminarea gaussiană, cu $\mu = \frac{a_2}{b_1}$:
\begin{align*}
	 & b_1 x_1 + c_1 x_2             & = d_1           \\
	 & (b_1 - \mu c_1) x_2 + c_2 x_3 & = d_2 - \mu d_1
\end{align*}

Necunoscuta $x_1$ a fost eliminată, și am rămas cu o ecuație similară cu prima.
Acest procedeu s-ar continua până la final, deci putem defini coeficienții
recursiv (coeficienții primesi ecuații nu se modifică):
\begin{align*}
	\mu & = \frac{a_i}{b_{i - 1}} \\
	b_i & = b_i - \mu c_{i - 1}   \\
	d_i & = d_i - \mu d_{i - 1}
\end{align*}

În final, soluția sistemului o găsim prin substituție înapoi:
\begin{align*}
	x_n & = \frac{d_n}{b_n}                 \\
	x_i & = \frac{d_i - c_i x_{i + 1}}{b_i}
\end{align*}

Algoritmul în MATLAB îl construim folosind 4 vectori corespunzători
coeficienților și termenilor liberi.

\begin{algorithm}[h!]
	\caption{Thomas}
	\begin{algorithmic}[1]
		\State \( n \gets \text{lungimea vectorului } d \)
            \\
            \State \(x \gets 0_{n, 1}\)
            \\
		\For{$p = 2$ to $n$}
                \State \(\mu \gets a_i / b_{i-1}\)
                \State \(b_i \gets b_i - \mu * c_{i - 1}\)
                \State \(d_i \gets b_i - \mu * d_{i - 1}\)
		\EndFor
            \\
            \State \(x_n \gets d_n / b_n\)
            \For{$i = (n - 1)$ to $1$}
            \State \(x_i \gets (d_i - c_i * x_{i + 1}) / b_i\)
            \EndFor
	\end{algorithmic}
\end{algorithm}
%\octavescript{./src/Thomas.m}{}

Pentru analiza numerică, este suficient să analizăm ecuația:
\begin{equation*}
	b'_i = b_i - \frac{a_i c_{i - 1}}{b_{i - 1}}
\end{equation*}

După cum am observat în laboratoarele trecute, dacă $|b_{i - 1}|$ este foarte mic,
erorile se pot amplifica atunci când îl calculăm pe $\mu$. În consecință, ne-am
dori ca $|b_i| > |c_i|$. Mai mult, ne dorim ca $|b_i| > |a_i|$. O matrice care
satisface aceste condiții se numește \textit{diagonal dominantă}. Această
matrice este caracterizată de faptul ca elementele de pe diagonala principală
sunt mai mari decât suma elementelor de pe linie, mai exact, în cazul nostru:
\begin{equation*}
	|b_i| \geq |a_i| + |c_i|
\end{equation*}

\section{Probleme}

\begin{questions}
	\boxedpoints
	\pointsinmargin

	\question Fie sistemul de ecuații:
	\begin{align*}
		1.5  x_1 - 2.1  x_2    & = 8.3 \\
		-7.6   x_1 + 3.11  x_2 & = 6.7
	\end{align*}
	Rezolva\c{t}i sistemul folosind eliminare gaussian\u{a} cu pivotare par\c{t}ial\u{a}.


	\question Construiți o variantă modificată pentru algoritmul lui Thomas care
	să lucreze cu matricea:
	\begin{equation*}
		A = \begin{bmatrix}
			b_1 & 0      & c_1       &           &           &           & 0         \\
			0   & b_2    & 0         & c_2       &           &           &           \\
			a_3 & 0      & b_3       & 0         & c_3       &           &           \\
			    & \ddots & \ddots    & \ddots    & \ddots    & \ddots    &           \\
			    &        & a_{n - 2} & 0         & b_{n - 2} & 0         & c_{n - 2} \\
			    &        &           & a_{n - 1} & 0         & b_{n - 1} & 0         \\
			0   &        &           &           & a_n       & 0         & b_n       \\
		\end{bmatrix}
	\end{equation*}

	\question Fie sistemul de ecua\c{t}ii:
	\begin{equation*}
		\begin{cases}
			1.5  x_1 - 2.1 x_2 = 8.3 \\
			-7.6 x_1 + 3.11 x_2 = 6.7
		\end{cases}
	\end{equation*}
	Rezolvați sistemul folosind eliminare GPP, GPPS, GPT.

\end{questions}

\end{document}
